diff --git a/src/transnormer/models/train_model.py b/src/transnormer/models/train_model.py
index 6fb6182..50b4726 100644
--- a/src/transnormer/models/train_model.py
+++ b/src/transnormer/models/train_model.py
@@ -254,9 +254,11 @@ def train_seq2seq_model(
     # Set-up training arguments from hyperparameters
     training_args = transformers.Seq2SeqTrainingArguments(
         output_dir=output_dir,
-        predict_with_generate=True,
+        #predict_with_generate=True,
         num_train_epochs=configs["training_hyperparams"]["epochs"],
         per_device_train_batch_size=configs["training_hyperparams"]["batch_size"],
+        gradient_accumulation_steps=configs["training_hyperparams"]["gradient_accumulation_steps"],
+        log_level="info",
         per_device_eval_batch_size=configs["training_hyperparams"]["batch_size"],
         fp16=configs["training_hyperparams"]["fp16"],
         group_by_length=True,
@@ -294,6 +296,7 @@ def train_seq2seq_model(
 
     # Run training
     trainer.train()
+    trainer.save_model(output_dir)
 
     return None
 
@@ -310,7 +313,7 @@ if __name__ == "__main__":
     # Load configs
     with open(CONFIGFILE, mode="rb") as fp:
         CONFIGS = tomli.load(fp)
-    MODELDIR = os.path.join(ROOT, "./models/model")
+    MODELDIR = CONFIGS["training_hyperparams"]["output_dir"]
 
     # Fix seeds for reproducibilty
     random.seed(CONFIGS["random_seed"])
diff --git a/src/transnormer/models/generate.py b/src/transnormer/models/generate.py
index 5c59e71..d1bc4eb 100644
--- a/src/transnormer/models/generate.py
+++ b/src/transnormer/models/generate.py
@@ -149,7 +149,9 @@ def main(arguments: Optional[List[str]] = None) -> None:
     ds["train"] = ds["train"].remove_columns(index_column)
 
     # (7) Save outputs
-    ds["train"].to_json(args.out, force_ascii=False)
+    with open(args.out, 'w') as f:
+        for row in ds["train"]:
+            print(row["pred"], file=f)
 
 
 if __name__ == "__main__":
